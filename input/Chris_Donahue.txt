Professor: Chris Donahue
University: Carnegie Mellon University
URL: https://chrisdonahue.com/
Description: Chris Donahue
About
News
Group
Papers
Chris Donahue
Dannenberg Assistant Professor
Computer Science Department, Carnegie Mellon University
About

Email: (click to unscramble) r@uccnohudimhedua.se

I am an Assistant Professor in the Computer Science Department at Carnegie Mellon University, and also a Research Scientist at Google DeepMind on the Magenta team (part-time).

My research goal is to develop and responsibly deploy generative AI for music and creativity, thereby unlocking and augmenting human creative potential. To this end, my work involves (1) improving machine learning methods for controllable generative modeling for music, audio, and other sequential data, and (2) deploying real-world interactive systems that allow a broader audienceâ€”inclusive of non-musiciansâ€”to harness generative music AI through intuitive controls.

I am particularly drawn to research ideas with direct real-world applications, and my work often involves building systems for real users to be evaluated in-the-wild. For example, my work on Piano Genie was used in a live performance by The Flaming Lips, and my work on Dance Dance Convolution powers Beat Sage, a live service used by thousands of users a day to create multimodal music game content.

Previously, I was a postdoc at Stanford CS advised by Percy Liang. Before that, I completed a PhD at UCSD co-advised by Miller Puckette and Julian McAuley.

News
ğŸ“œ (Jun 2024) Two papers accepted to ISMIR 2024.
ğŸª‘ (Apr 2024) Named as the Dannenberg Assistant Professor of Computer Science.
ğŸ›ï¸ (Apr 2024) Serving as Senior Program Committee Co-chair for ISMIR 2024 - record number of submissions (415).
ğŸŒ (Mar 2024) A Copilot-like tool for musicians featuring the Anticipatory Music Transformer was launched in beta.
ğŸ“œ (Mar 2024) Music ControlNet to appear in TASLP (IEEE/ACM Transactions on Audio, Speech, and Language Processing).
ğŸ“œ (Mar 2024) Anticipatory Music Transformer to appear in TMLR (Transactions on Machine Learning Research).
ğŸŒ (Mar 2024) Launch of MusicFX DJ Mode, a real-time music audio generation tool developed by my team at Google.
ğŸ—ï¸ (Nov 2023) SingSong incorporated into Google DeepMindâ€™s Music AI Tools.
ğŸ“œ (Nov 2023) Work presented at HCMIR Workshop by Michael Feffer.
ğŸ“œ (Nov 2023) New preprint on controllable music gen led by Shih-Lun Wu (applying to PhD positions!)
ğŸ—ï¸ (Oct 2023) Interviewed for Pitchfork article about MusicLM
ğŸ¤ (Oct 2023) Invited talk at Stanford HAI Conference (recording, slides)
ğŸ§‘â€ğŸ« (Oct 2023) Guest lecture for CMU LLM Course (slides)
ğŸ‘‹ (Oct 2023) New PhD students: Irmak Bukey and Wayne Chi
ğŸ§‘â€ğŸ« (Sep 2023) Started as Assistant Professor at CMU
ğŸ“œ (Jun 2023) New preprint led by John Thickstun: Anticipatory Music Transformer
G-CLef

I lead the Generative Creativity Lab (G-CLef) at CMU. Our research focuses on the development and deployment of generative AI towards augmenting human creativity. We primarily focus on musical creativity as an application domain but also explore other areas such as gaming.

PhD students

Irmak Bukey
CSD PhD student
Wayne Chi
CSD PhD student

Affiliates

Alexander Wang
Music and Technology MS student
Michael Feffer
S3D PhD student
Shih-Lun Wu
LTI MS student
Yewon Kim
Visiting researcher, KAIST

Supporters

The G-CLef lab is supported by generous contributions from:

Recent Papers
Quickly discover relevant content by filtering publications.
 Shih-Lun Wu, Chris Donahue, Shinji Watanabe, Nicholas J. Bryan (2023). Music ControlNet: Multiple Time-varying Controls for Music Generation.

arXiv PDF BibTeX ğŸ”Š Examples Video

 Michael Feffer, Zachary C. Lipton, Chris Donahue (2023). DeepDrake ft. BTS-GAN and TayloRVC: An Exploratory Analysis of Musical Deepfakes and Hosting Platforms. In HCMIR.

PDF BibTeX Dataset

 John Thickstun, David Hall, Chris Donahue, Percy Liang (2023). Anticipatory Music Transformer.

arXiv PDF BibTeX ğŸ”Š Examples Code

 Chris Donahue, Antoine Caillon, Adam Roberts, Ethan Manilow, Philippe Esling, Andrea Agostinelli, Mauro Verzetti, Ian Simon, Olivier Pietquin, Neil Zeghidour, Jesse Engel (2023). SingSong: Generating Musical Accompaniments from Singing.

arXiv PDF BibTeX ğŸ”Š Examples

 Chris Donahue, John Thickstun, Percy Liang (2022). Melody Transcription via Generative Pre-training. In ISMIR.

arXiv PDF BibTeX ğŸ”Š Examples Code Dataset Video

SEE ALL PUBLICATIONS 

