Professor: Danqi Chen 0001
University: Princeton University
URL: https://www.cs.princeton.edu/~danqic
Description: 


Danqi Chen 

How to pronounce?

Assistant Professor of Computer Science
Princeton NLP Group
Princeton Language and Intelligence Princeton AIML Group

Email: danqic@cs.princeton.edu
Office: Computer Science 412









Papers / Lab / Teaching & Service / Awards / Etc

I am an assistant professor of Computer Science at Princeton University and I co-lead the Princeton NLP Group. I am also the associate director of Princeton Language and Intelligence, an initiative that seeks to develop fundamental research of large AI models (e.g., LLMs). Previously, I was a visiting scientist at Facebook AI Research (FAIR) in Seattle. I received my Ph.D. in Computer Science from Stanford University in 2018, where I was advised by Christopher Manning and worked in the Stanford NLP Group. Before that, I was an undergraduate student from the Special Pilot CS Class supervised by Andrew Yao at Tsinghua University.

Research

My research interests lie broadly in natural language processing and machine learning. I am always excited about simple and principled approaches that are practical, scalable, and generalizable in real-world problems.

These days, I am mostly drawn by the development of large language models. A list of topics that I am actively thinking about/working on:

I believe retrieval should play a fundamental role in next-generation language models to improve their factuality, adaptability, interpretability, and trustworthiness. We are actively exploring how to build effective retrievers and how to integrate retrieval with language models to achieve optimal trade-offs.
I am passionate about techniques that can potentially democratize training and deployment of large language models (especially making them accessible in academia), ranging from improved training approaches, data curation, optimization to model compression and downstream adaptations.
I am also interested in research that advances our understanding of current language models (capabilities and limitations), both empirically and theoretically.

Education / Experience
2012-2018: Ph.D. in Computer Science, Stanford University
2008-2012: B.Eng in Computer Science, Tsinghua University
2010 Fall: Exchange student at Hong Kong University of Science and Technology
2019/2-2019/8: Visiting scientist at Facebook AI Research, Seattle
2019/2-2019/8: Visiting researcher at University of Washington, Seattle
2016/10-2016/12: Research intern at Facebook AI Research, NYC
2014/7-2014/9: Research intern at Microsoft Research, Redmond
2011/2-2012/5: Research intern at Microsoft Research Asia, Beijing
Contact

Prospective students: I am actively looking for strong and motivated students to join our group! If you are interested in working with me, please apply through the Princeton CS PhD program and mention me in your application (and I can't respond to individual emails).

Princeton students: (1) If you are already a graduate student at Princeton, feel free to reach out. (2) Undergraduate students: Considering my bandwidth and current group size, I won't be able to take on more undergraduate advisees for senior thesis and independent work in AY23-24 unfortunately. You are always welcome to reach out directly to my graduate students and work with them (a good starting point is to check out our recent publications, and send them an email/schedule a chat!). We also strongly encourage you take COS324/COS484 first, and also fill out this form before you reach out.



Last update: June 2024

