Professor: Nicolas Loizou
University: Johns Hopkins University
URL: https://nicolasloizou.github.io/
Description: About
 
News
 
Publications
 
Team
 
Talks
 
Bio
 
Professional Services
 
Teaching
 
Open Positions


	
Nicolas Loizou
Assistant Professor
Johns Hopkins University

Contact info:
Johns Hopkins University,
Department of Applied Mathematics and Statistics,
3400 N. Charles Street, Baltimore, MD 21218.
E-mail: nloizou@jhu.edu

           






About

I am an Assistant Professor in the Department of Applied Mathematics and Statistics (AMS) and the Mathematical Institute for Data Science (MINDS), with a secondary appointment in the Department of Computer Science at Johns Hopkins University. I am also affiliated with the JHU Machine Learning Group and the Ralph O’Connor Sustainable Energy Institute (ROSEI).

My research interests include large-scale optimization, machine learning, randomized numerical linear algebra, distributed and decentralized algorithms, game theory, and deep learning. My current research focuses on the theory and applications of convex and non-convex optimization in large-scale machine learning and data science problems.

Prior to this, I was an IVADO postdoctoral fellow at Mila - Quebec Artificial Intelligence Institute and DIRO, UdeM, where I worked closely with Simon Lacoste-Julien and Ioannis Mitliagkas. I obtained my PhD from The University of Edinburgh, School of Mathematics in 2019, under the supervision of Peter Richtarik. Before that, I spent 4 beautiful years in Athens as undergraduate student in the Department of Mathematics at National and Kapodistrian University of Athens and 1 year as postgraduate student at Imperial College London where I obtained an MSc in Computing. During the fall of 2018, I was a research intern at Facebook AI Research in Montreal, Canada.

For more details please feel free to look my CV (updated August 2023).


Selected Recent News
(For the full list of news please check the News tab.)

06 May 2024: This week, I am visting the Department of Computing at Imperial College London. On Tuesday, I will give a talk at the Computational Optimisation Seminar on "Next-Generation Adaptive Optimization Algorithms for Large-Scale Machine Learning Models".

I had a great time catching up with Panos Parpas, Wolfram Wiesemann, and Ruth Misener!

02-04 May 2024: I am attending AISTATS 2024 in Valencia, Spain.

Konstantinos is also here, presenting our work: Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities.

22-24 Mar 2024: I am attending 2024 INFORMS Optimization Society Conference (IOS 2023) at Houston, Texas. On Sunday, 24 March 2024, I will be presenting some of our latest findings on adaptive optimization algorithms. Title of the talk: "Advancing SGD: Exploring the Interplay of Stochastic Polyak Step-size and Momentum."

19 Jan 2024: Our paper "Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities", joint work with Konstantinos Emmanouilidis and Rene Vidal, was accepted to AISTATS 2024.

Special congrats to Konstantinos Emmanouilidis on the acceptance of his first paper!

16 Jan 2024: Our paper Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates, joint work with Siqi Zhang, Sayantan Choudhury, and Sebastian U Stich was accepted to ICLR 2024.

10 Dec 2023: I am traveling to New Orleans to attend NeurIPS 2023.

Sayantan Choudhury is also here, presenting our work: Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions.

15 Nov 2023: This week, I am co-organizing the DeepMath 2023 Conference on the Mathematical Theory of Deep Neural Networks, which takes place at JHU. I am looking forward to the excellent set of speakers and the poster sessions.

18 Oct 2023: This week, I am visting the Combinatorics and Optimization Department at the University of Waterloo. On Friday, 20 October, I will give a talk at the Continous Optimization Seminar on "Next-Generation Adaptive Optimization Algorithms for Large-Scale Machine Learning Models".

Steve Vavasis and Yaoliang Yu thanks for the great hospitality!

22 Sept 2023: Our paper Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions,
joint work with Sayantan Choudhury, and Eduard Gorbunov was accepted to NeurIPS 2023.

Special congrats to Sayantan Choudhury on the acceptance of his first paper!

18 Sept 2023: I am visiting Flatiron Institute in New York City this week.

28 Aug. 2023:

New Phd Student: Dimitris Oikonomou

Dimitris Oikonomou is a new member in my Lab. He has just started his Ph.D. in the CS department.

Dimitris got his BSc in Mathematics from the National and Kapodistrian University of Athens. He then obtained two MSE degrees: MSc in Mathematics from the University of Gottingen, Germany and MSc in Data Science and Machine Learning from National Technical University of Athens. All BSc and MSc degrees with distinction!

He was also very active in Mathematical Olympiads competitions:

International Mathematical Olympiad (IMO): Bronze Medal (Colombia 2013)
Greek Mathematical Olympiad: Silver Medal (2014)
Greek Mathematical Olympiad: Bronze Medal (2013)

Dimitri, welcome to the team!

21 Aug 2023: Our paper Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization , joint work with Ahmed Khaled, Othmane Sebbouh, Robert M. Gower, and Peter Richtárik was accepted to Journal of Optimization Theory and Applications (JOTA)

12 Jul 2023: New Paper out: Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes, joint work with Sohom Mukherjee, and Sebastian U Stich.

08 Jun 2023: New Paper out: Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates , joint work with Siqi Zhang, Sayantan Choudhury, and Sebastian U Stich.

31 May 2023: Attending SIAM Conference on Optimization (OP23) in Seattle this week! Reach out if you would like to meet. On June 1, with Siqi Zhang and Sayantan Choudhury , we organize a 4-session mini-symposium on "Recent Advancements in Optimization Methods for Machine Learning".

22-24 Mar 2023: I am attending 57th Annual Conference on Information Science and Systems (CISS 2023) at JHU. I am organizing a session on Optimization Methods for Machine Learning and giving a talk in the Estimation & Learning in Stochastic Systems session organized by James Spall.

03 March 2023: CISCO Research Gift: Thrilled to receive a CISCO Research monetary gift for working on the foundations of Multi-player Federated Learning.

27 Feb 2023: New Paper out: Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions , joint work with Sayantan Choudhury and Eduard Gorbunov.

22-24 Feb 2023: I am attending CSLSC 2023 at the University of Illinois at Urbana-Champaign this week. I will give the keynote presentation at the Optimization, Control and Reinforcement Learning Session on Friday, 24 Feb. Please reach out if you want to meet.

20 Feb 2023: Our paper AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods , joint work with Zheng Shi, Abdurakhmon Sadiev, Peter Richtárik, and Martin Takáč was accepted to Transactions on Machine Learning Research (TMLR)

20 Jan 2023: AISTATS 2023 and ICLR 2023 decisions are out:

Our paper Stochastic Gradient Descent-Ascent: Unified Theory and New Efficient Methods, joint work with Aleksandr Beznosikov, Eduard Gorbunov, and Hugo Berard, was accepted to AISTATS 2023.

Our paper A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games, joint work with Samuel Sokota, Ryan D'Orazio, J Zico Kolter, Marc Lanctot, Ioannis Mitliagkas, Noam Brown, and Christian Kroer was accepted to ICLR 2023.

14 Dec 2022: Since 05 Dec 2022, I have been a research visitor at Vector Institute in Toronto. If you are a student or researcher in Toronto, please feel free to drop me a message. I would be happy to arrange a meeting. I will be here until 18 Jan 2023.

28 Nov 2022: I am attending NeurIPS 2022 this week. Please drop me a message if you would like to meet.

We are presenting our work Dynamics of SGD with Stochastic Polyak Stepsizes: Truly Adaptive Variants and Convergence to Exact Solution in the main conference. Please join us during NeurIPS' first poster session tomorrow (29 Nov).

Three more papers have been accepted at NeurIPS workshops.

14 Sept 2022: Our paper Dynamics of SGD with Stochastic Polyak Stepsizes: Truly Adaptive Variants and Convergence to Exact Solution , joint work with Antonio Orvieto and Simon Lacoste-Julien, was accepted to NeurIPS 2022.

29 Aug 2022: The Fall 2022 semester begins. I am teaching the new graduate course, "EN.553.669: Large Scale Optimization for Data Science," offered for the first time at JHU this year.

26 Jul 2022: I am attending The seventh International Conference on Continuous Optimization (ICCOPT) this week. Today I am presenting our recent work Stochastic Gradient Descent-Ascent: Unified Theory and New Efficient Methods.
ICCOPT 2022 is having twelve clusters this year. Sebastian Stich and I are the cluster chairs of the Optimization for Data Science and Machine Learning cluster. You are all welcome to our sessions!

17 Jul 2022: I am attending ICML 2022. this week, which conveniently is taking place in Baltimore this year. Please drop me a message if you are around, and we can catch up.

22 Apr 2022: I have been selected as a “Highlighted Reviewer of ICLR 2022”. All reviewers are listed on the ICLR website.

08 Mar 2022: I am giving a talk at MINDS/CIS Seminar Series at JHU. My talk is entitled “Stochastic Iterative Methods for Smooth Games: Practical Variants and Convergence Guarantees" and a recording is available here.

18 Jan 2022: Three papers were accepted to AISTATS 2022 (25th International Conference on Artificial Intelligence and Statistics):

Stochastic Extragradient: General Analysis and Improved Rates,
joint work with Eduard Gorbunov, Hugo Berard, and Gauthier Gidel.

Extragradient Method: O(1/K) Last-Iterate Convergence for Monotone Variational Inequalities and Connections With Cocoercivity
joint work with Eduard Gorbunov, and Gauthier Gidel.

On the Convergence of Stochastic Extragradient for Bilinear Games with Restarted Iteration Averaging
joint work with Chris Junchi Li, Yaodong Yu, Gauthier Gidel, Yi Ma, Nicolas Le Roux, Michael I Jordan.

07 Jan 2022: I officially started as an Assistant Professor in the Department of Applied Mathematics and Statistics (AMS) and the Mathematical Institute for Data Science (MINDS), at Johns Hopkins University!!!

06 Dec 2021: I am attending NeurIPS 2021, this week.
I am presenting our work Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity.

25 Oct 2021: I am attending 2021 INFORMS Annual Meeting this week.
Today I am presenting our recent work Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity at the virtual session Recent Advances in Stochastic Gradient Algorithms.

05 Oct 2021: 2020 COAP Best Paper Award
Our paper Momentum and Stochastic Momentum for Stochastic Gradient, Newton, Proximal Point and Subspace Descent Methods, published in Computational Optimization and Applications (COAP) was voted by the editorial board as the best paper appearing in the journal in 2020!

28 Sept 2021: Our paper Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity, joint work with Hugo Berard, Gauthier Gidel, Ioannis Mitliagkas and Simon Lacoste-Julien, was accepted to NeurIPS 2021

29 Apr 2021: Our paper "Revisiting Randomized Gossip Algorithms: General Framework, Convergence Rates and Novel Block and Accelerated Protocols", joint work with Peter Richtarik, was accepted to IEEE Transactions on Information Theory.

22 Jan 2021: Two papers were accepted to AISTATS 2021 (24th International Conference on Artificial Intelligence and Statistics):

Stochastic Polyak step-size for SGD: An adaptive learning rate for fast convergence
joint work with Sharan Vaswani, Issam Laradji and Simon Lacoste-Julien.

SGD for structured nonconvex functions: Learning rates, minibatching and interpolation
joint work with Robert M. Gower and Othmane Sebbouh.

16 Oct 2020: I am delighted to be selected as the runner-up of the OR Society’s Doctoral Award for 2019. This is an award for the "Most Distinguished Body of Research leading to the Award of a Doctorate in the field of Operational Research" in the United Kingdom.

19 Aug 2020: Our paper "Convergence Analysis of Inexact Randomized Iterative Methods", joint work with Peter Richtarik, was accepted to SIAM Journal on Scientific Computing (SISC)

19 Aug 2020: Our paper "Momentum and Stochastic Momentum for Stochastic Gradient, Newton, Proximal Point and Subspace Descent Methods ", joint work with Peter Richtarik, was accepted to Computational Optimization and Applications (COAP)

20 Jun 2020: New Paper out: Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization , joint work with Ahmed Khaled, Othmane Sebbouh, Robert M. Gower and Peter Richtárik.

18 Jun 2020: New Paper out: SGD for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation, joint work with Robert M. Gower and Othmane Sebbouh.

01 Jun 2020: Two papers were accepted to ICML 2020 (37th International Conference on Machine Learning):

A Unified Theory of Decentralized SGD with Changing Topology and Local Updates
joint work with Anastasia Koloskova, Sadra Boreiri, Martin Jaggi and Sebastian U. Stich.

Stochastic Hamiltonian Gradient Methods for Smooth Games
joint work with Hugo Berard, Alexia Jolicoeur-Martineau, Pascal Vincent, Simon Lacoste-Julien and Ioannis Mitliagkas.

23 Mar 2020: New Paper out: A Unified Theory of Decentralized SGD with Changing Topology and Local Updates, joint work with Anastasia Koloskova, Sadra Boreiri, Martin Jaggi and Sebastian U. Stich.

24 Feb 2020: New Paper out: Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence, joint work with Sharan Vaswani, Issam Laradji and Simon Lacoste-Julien.

19 December 2019: I am delighted to be awarded the IVADO Fellow Postdoctoral Scholarship .
For more details on The Institute for Data Valorisation (IVADO) and its mission check out the IVADO's website

Credits for website's template : Peter Richtarik

